{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":329006,"sourceType":"datasetVersion","datasetId":139630},{"sourceId":2061927,"sourceType":"datasetVersion","datasetId":1235849},{"sourceId":3799432,"sourceType":"datasetVersion","datasetId":2265304},{"sourceId":13877601,"sourceType":"datasetVersion","datasetId":8841658}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn # Neural Network\nimport torch.optim as optim # Optimizer's Relu etc. \nfrom torch.utils.data import Dataset, DataLoader # Preprocessing Dataset, Loading Dataset\n\nfrom PIL import Image # Open Image\nimport torchvision.transforms as transforms # Image Processing & Augmentation i.e. Resize, Flip, Normalize etc. \n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics.pairwise import cosine_similarity # Measures Similarity Between Vectors \nimport pandas as pd\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:25.405982Z","iopub.execute_input":"2026-01-15T17:42:25.406473Z","iopub.status.idle":"2026-01-15T17:42:28.472456Z","shell.execute_reply.started":"2026-01-15T17:42:25.406422Z","shell.execute_reply":"2026-01-15T17:42:28.471009Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df_images = pd.read_csv('/kaggle/input/fashion-product-images-dataset/fashion-dataset/images.csv')\n\ndf_styles = pd.read_csv(\n    '/kaggle/input/fashion-product-images-dataset/fashion-dataset/styles.csv',\n    on_bad_lines='skip',\n    encoding='utf-8'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:43:57.977505Z","iopub.execute_input":"2026-01-15T17:43:57.978595Z","iopub.status.idle":"2026-01-15T17:43:58.014941Z","shell.execute_reply.started":"2026-01-15T17:43:57.978509Z","shell.execute_reply":"2026-01-15T17:43:58.012874Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_19/1812054907.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/fashion-product-images-dataset/fashion-dataset/images.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m df_styles = pd.read_csv(\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'/kaggle/input/fashion-product-images-dataset/fashion-dataset/styles.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mon_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'skip'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/fashion-product-images-dataset/fashion-dataset/images.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/fashion-product-images-dataset/fashion-dataset/images.csv'","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"df_styles.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.619363Z","iopub.status.idle":"2026-01-15T17:42:28.620122Z","shell.execute_reply.started":"2026-01-15T17:42:28.619819Z","shell.execute_reply":"2026-01-15T17:42:28.619847Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_styles['masterCategory']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.622775Z","iopub.status.idle":"2026-01-15T17:42:28.623203Z","shell.execute_reply.started":"2026-01-15T17:42:28.623015Z","shell.execute_reply":"2026-01-15T17:42:28.623035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_styles['subCategory'].unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.624356Z","iopub.status.idle":"2026-01-15T17:42:28.624780Z","shell.execute_reply.started":"2026-01-15T17:42:28.624586Z","shell.execute_reply":"2026-01-15T17:42:28.624607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_styles['masterCategory']\ndf_masterCategory_OnlyApparel = df_styles[df_styles['masterCategory']=='Apparel']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.626484Z","iopub.status.idle":"2026-01-15T17:42:28.626925Z","shell.execute_reply.started":"2026-01-15T17:42:28.626727Z","shell.execute_reply":"2026-01-15T17:42:28.626747Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_masterCategory_OnlyApparel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.628657Z","iopub.status.idle":"2026-01-15T17:42:28.629044Z","shell.execute_reply.started":"2026-01-15T17:42:28.628861Z","shell.execute_reply":"2026-01-15T17:42:28.628881Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_masterCategory_OnlyApparel['subCategory'].unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.630563Z","iopub.status.idle":"2026-01-15T17:42:28.630943Z","shell.execute_reply.started":"2026-01-15T17:42:28.630771Z","shell.execute_reply":"2026-01-15T17:42:28.630788Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df_masterCategory_OnlyApparel['subCategory'] == df_styles['Bottomwear']\ndf_subCategory_OnlyApparel_Bottomwear = df_masterCategory_OnlyApparel['subCategory']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.632074Z","iopub.status.idle":"2026-01-15T17:42:28.632444Z","shell.execute_reply.started":"2026-01-15T17:42:28.632268Z","shell.execute_reply":"2026-01-15T17:42:28.632286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_styles.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.633883Z","iopub.status.idle":"2026-01-15T17:42:28.634359Z","shell.execute_reply.started":"2026-01-15T17:42:28.634108Z","shell.execute_reply":"2026-01-15T17:42:28.634126Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_updated_styles = df_styles[df_styles['gender'].isin(['Men', 'Women'])]\ndf_updated_styles","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.635474Z","iopub.status.idle":"2026-01-15T17:42:28.635886Z","shell.execute_reply.started":"2026-01-15T17:42:28.635695Z","shell.execute_reply":"2026-01-15T17:42:28.635712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_images.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.637724Z","iopub.status.idle":"2026-01-15T17:42:28.638105Z","shell.execute_reply.started":"2026-01-15T17:42:28.637922Z","shell.execute_reply":"2026-01-15T17:42:28.637941Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_images.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.639315Z","iopub.status.idle":"2026-01-15T17:42:28.639718Z","shell.execute_reply.started":"2026-01-15T17:42:28.639506Z","shell.execute_reply":"2026-01-15T17:42:28.639524Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Quick overview\nprint(\"Number of products in styles.csv:\", len(df_styles))\nprint(\"Number of images in images.csv:\", len(df_images))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.641016Z","iopub.status.idle":"2026-01-15T17:42:28.641377Z","shell.execute_reply.started":"2026-01-15T17:42:28.641200Z","shell.execute_reply":"2026-01-15T17:42:28.641218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Styles CSV Shape\", df_styles.shape)\nprint(\"Images CSV Shape\", df_images.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.642929Z","iopub.status.idle":"2026-01-15T17:42:28.643290Z","shell.execute_reply.started":"2026-01-15T17:42:28.643116Z","shell.execute_reply":"2026-01-15T17:42:28.643134Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"total_products = df_styles['id'].nunique()\nprint(\"Total unique products:\", total_products)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.645218Z","iopub.status.idle":"2026-01-15T17:42:28.645638Z","shell.execute_reply.started":"2026-01-15T17:42:28.645422Z","shell.execute_reply":"2026-01-15T17:42:28.645440Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Merging Image with Meta Data","metadata":{}},{"cell_type":"code","source":"# Total images\ndf = df_styles\nprint(\"Total images:\", len(df))\n\n# Number of unique categories\nprint(\"Master categories:\", df['masterCategory'].nunique())\nprint(\"Subcategories:\", df['subCategory'].nunique())\nprint(\"Article types:\", df['articleType'].nunique())\nprint(\"Colors:\", df['baseColour'].nunique())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.647633Z","iopub.status.idle":"2026-01-15T17:42:28.648030Z","shell.execute_reply.started":"2026-01-15T17:42:28.647840Z","shell.execute_reply":"2026-01-15T17:42:28.647859Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Class Distribution (Master Category)","metadata":{}},{"cell_type":"code","source":"# Count images per masterCategory\ncategory_counts = df['masterCategory'].value_counts()\n\nplt.figure(figsize=(10,5))\nsns.barplot(x=category_counts.index, y=category_counts.values)\nplt.title(\"Number of images per Master Category\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Master Category\")\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.649479Z","iopub.status.idle":"2026-01-15T17:42:28.649861Z","shell.execute_reply.started":"2026-01-15T17:42:28.649687Z","shell.execute_reply":"2026-01-15T17:42:28.649705Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Subcategory Distribution","metadata":{}},{"cell_type":"code","source":"# Top 20 subcategories\nsub_counts = df['subCategory'].value_counts().head(20)\n\nplt.figure(figsize=(12,6))\nsns.barplot(x=sub_counts.index, y=sub_counts.values)\nplt.title(\"Top 20 Subcategories\")\nplt.xticks(rotation=90)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.651209Z","iopub.status.idle":"2026-01-15T17:42:28.651606Z","shell.execute_reply.started":"2026-01-15T17:42:28.651406Z","shell.execute_reply":"2026-01-15T17:42:28.651423Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Color distribution","metadata":{}},{"cell_type":"code","source":"color_counts = df['baseColour'].value_counts().head(15)\n\nplt.figure(figsize=(12,5))\nsns.barplot(x=color_counts.index, y=color_counts.values)\nplt.title(\"Top 15 Colors\")\nplt.xticks(rotation=45)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.653566Z","iopub.status.idle":"2026-01-15T17:42:28.653958Z","shell.execute_reply.started":"2026-01-15T17:42:28.653777Z","shell.execute_reply":"2026-01-15T17:42:28.653796Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Number of Images Per Master Category","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.countplot(data=df, x='masterCategory', order=df['masterCategory'].value_counts().index)\nplt.title(\"Number of Images per Master Category\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Master Category\")\nplt.xticks(rotation=45)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.656095Z","iopub.status.idle":"2026-01-15T17:42:28.656450Z","shell.execute_reply.started":"2026-01-15T17:42:28.656279Z","shell.execute_reply":"2026-01-15T17:42:28.656296Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Number of Images per sub category","metadata":{}},{"cell_type":"code","source":"top_subcategories = df['subCategory'].value_counts().head(20)\nplt.figure(figsize=(12,6))\nsns.barplot(x=top_subcategories.index, y=top_subcategories.values)\nplt.title(\"Top 20 Subcategories by Image Count\")\nplt.xticks(rotation=90)\nplt.ylabel(\"Count\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.657498Z","iopub.status.idle":"2026-01-15T17:42:28.657899Z","shell.execute_reply.started":"2026-01-15T17:42:28.657721Z","shell.execute_reply":"2026-01-15T17:42:28.657740Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Color distribution (top 15)","metadata":{}},{"cell_type":"code","source":"top_colors = df['baseColour'].value_counts().head(15)\nplt.figure(figsize=(12,5))\nsns.barplot(x=top_colors.index, y=top_colors.values)\nplt.title(\"Top 15 Colors in Dataset\")\nplt.xticks(rotation=45)\nplt.ylabel(\"Count\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.659089Z","iopub.status.idle":"2026-01-15T17:42:28.659432Z","shell.execute_reply.started":"2026-01-15T17:42:28.659267Z","shell.execute_reply":"2026-01-15T17:42:28.659283Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Images per gender","metadata":{}},{"cell_type":"code","source":"gender_counts = df['gender'].value_counts()\nplt.figure(figsize=(6,4))\nsns.barplot(x=gender_counts.index, y=gender_counts.values)\nplt.title(\"Number of Images per Gender\")\nplt.ylabel(\"Count\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.661082Z","iopub.status.idle":"2026-01-15T17:42:28.661446Z","shell.execute_reply.started":"2026-01-15T17:42:28.661268Z","shell.execute_reply":"2026-01-15T17:42:28.661286Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Category vs. Gender Heatmap","metadata":{}},{"cell_type":"code","source":"category_gender = pd.crosstab(df['masterCategory'], df['gender'])\nplt.figure(figsize=(10,6))\nsns.heatmap(category_gender, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Category vs Gender Distribution\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.663074Z","iopub.status.idle":"2026-01-15T17:42:28.663419Z","shell.execute_reply.started":"2026-01-15T17:42:28.663256Z","shell.execute_reply":"2026-01-15T17:42:28.663272Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Subcategory vs. Color Heatmap (top 20 subcategories)","metadata":{}},{"cell_type":"code","source":"top_subs = df['subCategory'].value_counts().head(20).index\ndf_top = df[df['subCategory'].isin(top_subs)]\n\nsub_color = pd.crosstab(df_top['subCategory'], df_top['baseColour'])\nplt.figure(figsize=(12,6))\nsns.heatmap(sub_color, cmap=\"coolwarm\", annot=False)\nplt.title(\"Top 20 Subcategories vs Color Distribution\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.665071Z","iopub.status.idle":"2026-01-15T17:42:28.665415Z","shell.execute_reply.started":"2026-01-15T17:42:28.665245Z","shell.execute_reply":"2026-01-15T17:42:28.665261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"subcategory_counts = df_styles['subCategory'].value_counts()\nsubcategory_counts.head(10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.667070Z","iopub.status.idle":"2026-01-15T17:42:28.667456Z","shell.execute_reply.started":"2026-01-15T17:42:28.667267Z","shell.execute_reply":"2026-01-15T17:42:28.667285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nsubcategory_counts[:15].plot(kind='bar')\nplt.title(\"Top 15 SubCategories by Image Count\")\nplt.ylabel(\"Number of Images\")\nplt.xlabel(\"SubCategory\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.669090Z","iopub.status.idle":"2026-01-15T17:42:28.669687Z","shell.execute_reply.started":"2026-01-15T17:42:28.669386Z","shell.execute_reply":"2026-01-15T17:42:28.669413Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Filter Top Wear","metadata":{}},{"cell_type":"code","source":"topwear_df = df_styles[df_styles['subCategory'] == 'Topwear'].reset_index(drop=True)\n\nprint(\"Topwear images:\", topwear_df.shape[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.671475Z","iopub.status.idle":"2026-01-15T17:42:28.671880Z","shell.execute_reply.started":"2026-01-15T17:42:28.671703Z","shell.execute_reply":"2026-01-15T17:42:28.671722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\n\nbase_path = '/kaggle/input/fashion-product-images-dataset/fashion-dataset'\nstyles_df = df_styles\n# Parameters\nIMAGE_FOLDER = f\"{base_path}/images\"\ntop_n = 5  # number of images per subcategory\nsubcategories = ['Topwear', 'Shoes', 'Bags', 'Bottomwear', 'Watches']  # chosen for your project\n\nplt.figure(figsize=(20, 5))\n\nfor i, subcat in enumerate(subcategories):\n    sub_df = styles_df[styles_df['subCategory'] == subcat].sample(top_n, random_state=42)\n    \n    for j, row in enumerate(sub_df.itertuples()):\n        plt.subplot(len(subcategories), top_n, i*top_n + j + 1)\n        img_path = os.path.join(IMAGE_FOLDER, str(row.id) + \".jpg\")\n        img = Image.open(img_path)\n        plt.imshow(img)\n        plt.axis(\"off\")\n        if j == 0:\n            plt.ylabel(subcat, fontsize=14)\nplt.suptitle(\"Sample Images from Each SubCategory\", fontsize=25)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.673253Z","iopub.status.idle":"2026-01-15T17:42:28.673644Z","shell.execute_reply.started":"2026-01-15T17:42:28.673440Z","shell.execute_reply":"2026-01-15T17:42:28.673459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\n\nplt.figure(figsize=(12, 6))\nsns.barplot(x=subcategories, y=[styles_df[styles_df['subCategory']==sc].shape[0] for sc in subcategories])\nplt.title(\"Number of Images per SubCategory\")\nplt.ylabel(\"Number of Images\")\nplt.xlabel(\"SubCategory\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.675261Z","iopub.status.idle":"2026-01-15T17:42:28.675653Z","shell.execute_reply.started":"2026-01-15T17:42:28.675446Z","shell.execute_reply":"2026-01-15T17:42:28.675464Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Filtering Top Wear Articles ","metadata":{}},{"cell_type":"code","source":"topwear_df = styles_df[styles_df['subCategory'] == 'Topwear']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.677332Z","iopub.status.idle":"2026-01-15T17:42:28.677704Z","shell.execute_reply.started":"2026-01-15T17:42:28.677500Z","shell.execute_reply":"2026-01-15T17:42:28.677516Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"topwear_counts = topwear_df['articleType'].value_counts()\ntopwear_counts.head(10)  # show top 10 for sanity check\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.679039Z","iopub.status.idle":"2026-01-15T17:42:28.679405Z","shell.execute_reply.started":"2026-01-15T17:42:28.679226Z","shell.execute_reply":"2026-01-15T17:42:28.679245Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Take top 10 article types\ntop_10 = topwear_counts[:10]\n\nplt.figure(figsize=(12, 6))\nsns.barplot(x=top_10.index, y=top_10.values, palette=\"viridis\")\nplt.xticks(rotation=45)\nplt.title(\"Top 10 Topwear Article Types and Their Counts\")\nplt.xlabel(\"Article Type\")\nplt.ylabel(\"Number of Items\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.681043Z","iopub.status.idle":"2026-01-15T17:42:28.681387Z","shell.execute_reply.started":"2026-01-15T17:42:28.681213Z","shell.execute_reply":"2026-01-15T17:42:28.681238Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Filtering Top Wear Articles with respect to 'Gender'","metadata":{}},{"cell_type":"code","source":"topwear_gender_counts = topwear_df.groupby(['articleType', 'gender']).size().reset_index(name='count')\ntopwear_gender_counts.head(10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.682400Z","iopub.status.idle":"2026-01-15T17:42:28.682762Z","shell.execute_reply.started":"2026-01-15T17:42:28.682589Z","shell.execute_reply":"2026-01-15T17:42:28.682607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(12, 6))\nsns.barplot(data=topwear_gender_counts, x='articleType', y='count', hue='gender', palette='Set2')\nplt.xticks(rotation=45)\nplt.title(\"Topwear Article Types Distribution by Gender\")\nplt.xlabel(\"Article Type\")\nplt.ylabel(\"Number of Items\")\nplt.legend(title='Gender')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.683815Z","iopub.status.idle":"2026-01-15T17:42:28.684329Z","shell.execute_reply.started":"2026-01-15T17:42:28.684090Z","shell.execute_reply":"2026-01-15T17:42:28.684123Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train Test Validation Split\n### Train 70% \n### Validation 15%\n### Test 15%","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\napparel_df = styles_df[\n    styles_df['subCategory'].isin(['Topwear', 'Bottomwear'])\n]\n\ntrain_df, temp_df = train_test_split(\n    apparel_df, test_size=0.3, stratify=apparel_df['subCategory'], random_state=42\n)\n\nval_df, test_df = train_test_split(\n    temp_df, test_size=0.5, stratify=temp_df['subCategory'], random_state=42\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.686232Z","iopub.status.idle":"2026-01-15T17:42:28.686633Z","shell.execute_reply.started":"2026-01-15T17:42:28.686429Z","shell.execute_reply":"2026-01-15T17:42:28.686447Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\nbase_path = \"/kaggle/input/fashion-product-images-dataset/fashion-dataset\"\n\nstyles_df = pd.read_csv(\n    f\"{base_path}/styles.csv\",\n    engine=\"python\",\n    on_bad_lines=\"skip\"\n)\n\nprint(styles_df.shape)\nstyles_df.head()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.687898Z","iopub.status.idle":"2026-01-15T17:42:28.688234Z","shell.execute_reply.started":"2026-01-15T17:42:28.688072Z","shell.execute_reply":"2026-01-15T17:42:28.688089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"styles_df.columns\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.689397Z","iopub.status.idle":"2026-01-15T17:42:28.689773Z","shell.execute_reply.started":"2026-01-15T17:42:28.689597Z","shell.execute_reply":"2026-01-15T17:42:28.689615Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Remove Missing Id's ","metadata":{}},{"cell_type":"code","source":"styles_df.dropna(subset=['id'], inplace=True)\nstyles_df['id'] = styles_df['id'].astype(int)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.691279Z","iopub.status.idle":"2026-01-15T17:42:28.691704Z","shell.execute_reply.started":"2026-01-15T17:42:28.691474Z","shell.execute_reply":"2026-01-15T17:42:28.691492Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Filter only Topwear & Bottomwear","metadata":{}},{"cell_type":"code","source":"apparel_df = styles_df[\n    styles_df['subCategory'].isin(['Topwear', 'Bottomwear'])\n].copy()\n\nlabel_map = {'Topwear': 0, 'Bottomwear': 1}\napparel_df['label'] = apparel_df['subCategory'].map(label_map)\napparel_df['image'] = apparel_df['id'].astype(str) + \".jpg\"\n\napparel_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.692591Z","iopub.status.idle":"2026-01-15T17:42:28.692947Z","shell.execute_reply.started":"2026-01-15T17:42:28.692774Z","shell.execute_reply":"2026-01-15T17:42:28.692792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"apparel_df['subCategory'].value_counts()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.695101Z","iopub.status.idle":"2026-01-15T17:42:28.695678Z","shell.execute_reply.started":"2026-01-15T17:42:28.695375Z","shell.execute_reply":"2026-01-15T17:42:28.695403Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train / Validation / Test Split (FINAL & CLEAN)","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, temp_df = train_test_split(\n    apparel_df,\n    test_size=0.3,\n    stratify=apparel_df['label'],\n    random_state=42\n)\n\nval_df, test_df = train_test_split(\n    temp_df,\n    test_size=0.5,\n    stratify=temp_df['label'],\n    random_state=42\n)\n\nprint(\"Train:\", train_df.shape)\nprint(\"Val:\", val_df.shape)\nprint(\"Test:\", test_df.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.697642Z","iopub.status.idle":"2026-01-15T17:42:28.698007Z","shell.execute_reply.started":"2026-01-15T17:42:28.697830Z","shell.execute_reply":"2026-01-15T17:42:28.697847Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Image Transforms (Basic, No Augmentation Yet)","metadata":{}},{"cell_type":"code","source":"from torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),  # smaller for faster training\n    transforms.ToTensor(),\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.699237Z","iopub.status.idle":"2026-01-15T17:42:28.700064Z","shell.execute_reply.started":"2026-01-15T17:42:28.699836Z","shell.execute_reply":"2026-01-15T17:42:28.699857Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Custom Dataset Class","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport os\n\nIMAGE_DIR = f\"{base_path}/images\"\n\nclass FashionDataset(Dataset):\n    def __init__(self, df, image_dir, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.image_dir = image_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.image_dir, self.df.loc[idx, 'image'])\n        image = Image.open(img_path).convert(\"RGB\")\n        label = self.df.loc[idx, 'label']\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.702095Z","iopub.status.idle":"2026-01-15T17:42:28.702724Z","shell.execute_reply.started":"2026-01-15T17:42:28.702404Z","shell.execute_reply":"2026-01-15T17:42:28.702432Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DataLoaders","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_ds = FashionDataset(train_df, IMAGE_DIR, transform)\nval_ds   = FashionDataset(val_df, IMAGE_DIR, transform)\ntest_ds  = FashionDataset(test_df, IMAGE_DIR, transform)\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=2)\nval_loader   = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=2)\ntest_loader  = DataLoader(test_ds, batch_size=32, shuffle=False, num_workers=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.704915Z","iopub.status.idle":"2026-01-15T17:42:28.705313Z","shell.execute_reply.started":"2026-01-15T17:42:28.705128Z","shell.execute_reply":"2026-01-15T17:42:28.705147Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FINAL CHECKPOINT (DO NOT SKIP)","metadata":{}},{"cell_type":"code","source":"images, labels = next(iter(train_loader))\nprint(images.shape)\nprint(labels[:10])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.706984Z","iopub.status.idle":"2026-01-15T17:42:28.707366Z","shell.execute_reply.started":"2026-01-15T17:42:28.707182Z","shell.execute_reply":"2026-01-15T17:42:28.707201Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Build CNN FROM SCRATCH (NO PRETRAINED)","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n\n        self.pool = nn.MaxPool2d(2, 2)\n\n        self.fc1 = nn.Linear(128 * 16 * 16, 256)\n        self.fc2 = nn.Linear(256, 2)  # 2 classes\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))  # 64x64\n        x = self.pool(F.relu(self.conv2(x)))  # 32x32\n        x = self.pool(F.relu(self.conv3(x)))  # 16x16\n\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.708440Z","iopub.status.idle":"2026-01-15T17:42:28.708824Z","shell.execute_reply.started":"2026-01-15T17:42:28.708645Z","shell.execute_reply":"2026-01-15T17:42:28.708663Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Setup","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = SimpleCNN().to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.710124Z","iopub.status.idle":"2026-01-15T17:42:28.710463Z","shell.execute_reply.started":"2026-01-15T17:42:28.710301Z","shell.execute_reply":"2026-01-15T17:42:28.710318Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train the CNN (CORE STEP)","metadata":{}},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, epochs=5):\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        correct = 0\n        total = 0\n\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n        train_acc = 100 * correct / total\n\n        model.eval()\n        val_correct = 0\n        val_total = 0\n\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                _, predicted = outputs.max(1)\n                val_total += labels.size(0)\n                val_correct += predicted.eq(labels).sum().item()\n\n        val_acc = 100 * val_correct / val_total\n\n        print(f\"Epoch [{epoch+1}/{epochs}] \"\n              f\"Loss: {train_loss/len(train_loader):.4f} \"\n              f\"Train Acc: {train_acc:.2f}% \"\n              f\"Val Acc: {val_acc:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.711786Z","iopub.status.idle":"2026-01-15T17:42:28.712178Z","shell.execute_reply.started":"2026-01-15T17:42:28.711995Z","shell.execute_reply":"2026-01-15T17:42:28.712015Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nimage_dir = f\"{base_path}/images\"\n\nexisting_images = set(os.listdir(image_dir))\nprint(\"Total images on disk:\", len(existing_images))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.713902Z","iopub.status.idle":"2026-01-15T17:42:28.714259Z","shell.execute_reply.started":"2026-01-15T17:42:28.714083Z","shell.execute_reply":"2026-01-15T17:42:28.714100Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"apparel_df['image'] = apparel_df['id'].astype(str) + \".jpg\"\n\n# Keep only rows where image exists\napparel_df = apparel_df[apparel_df['image'].isin(existing_images)].copy()\n\nprint(\"After cleaning:\", apparel_df.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.715400Z","iopub.status.idle":"2026-01-15T17:42:28.715796Z","shell.execute_reply.started":"2026-01-15T17:42:28.715603Z","shell.execute_reply":"2026-01-15T17:42:28.715623Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, temp_df = train_test_split(\n    apparel_df,\n    test_size=0.3,\n    stratify=apparel_df['label'],\n    random_state=42\n)\n\nval_df, test_df = train_test_split(\n    temp_df,\n    test_size=0.5,\n    stratify=temp_df['label'],\n    random_state=42\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.716936Z","iopub.status.idle":"2026-01-15T17:42:28.717292Z","shell.execute_reply.started":"2026-01-15T17:42:28.717117Z","shell.execute_reply":"2026-01-15T17:42:28.717133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ds = FashionDataset(train_df, image_dir, transform)\nval_ds   = FashionDataset(val_df, image_dir, transform)\ntest_ds  = FashionDataset(test_df, image_dir, transform)\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=2)\nval_loader   = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=2)\ntest_loader  = DataLoader(test_ds, batch_size=32, shuffle=False, num_workers=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.718857Z","iopub.status.idle":"2026-01-15T17:42:28.719221Z","shell.execute_reply.started":"2026-01-15T17:42:28.719045Z","shell.execute_reply":"2026-01-15T17:42:28.719064Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Try loading one image\nfrom PIL import Image\nimg_path = os.path.join(image_dir, train_df.iloc[0]['image'])\nImage.open(img_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.720782Z","iopub.status.idle":"2026-01-15T17:42:28.721153Z","shell.execute_reply.started":"2026-01-15T17:42:28.720974Z","shell.execute_reply":"2026-01-15T17:42:28.720994Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Starting Training ","metadata":{}},{"cell_type":"code","source":"train_model(model, train_loader, val_loader, epochs=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.722471Z","iopub.status.idle":"2026-01-15T17:42:28.722854Z","shell.execute_reply.started":"2026-01-15T17:42:28.722682Z","shell.execute_reply":"2026-01-15T17:42:28.722700Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Convert CNN -> Feature Extractor","metadata":{}},{"cell_type":"code","source":"class FeatureExtractor(nn.Module):\n    def __init__(self, trained_model):\n        super().__init__()\n        self.features = nn.Sequential(\n            trained_model.conv1,\n            nn.ReLU(),\n            trained_model.pool,\n            trained_model.conv2,\n            nn.ReLU(),\n            trained_model.pool,\n            trained_model.conv3,\n            nn.ReLU(),\n            trained_model.pool\n        )\n        self.fc = trained_model.fc1  # keep embedding layer\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.724037Z","iopub.status.idle":"2026-01-15T17:42:28.724385Z","shell.execute_reply.started":"2026-01-15T17:42:28.724210Z","shell.execute_reply":"2026-01-15T17:42:28.724236Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Initialize Feature Extractor","metadata":{}},{"cell_type":"code","source":"feature_model = FeatureExtractor(model).to(device)\nfeature_model.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.725581Z","iopub.status.idle":"2026-01-15T17:42:28.725916Z","shell.execute_reply.started":"2026-01-15T17:42:28.725754Z","shell.execute_reply":"2026-01-15T17:42:28.725770Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Extract Feature Vectors (CORE STEP)","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef extract_features(model, dataloader):\n    features = []\n    labels = []\n    images = []\n\n    with torch.no_grad():\n        for imgs, lbls in dataloader:\n            imgs = imgs.to(device)\n            emb = model(imgs)\n            features.append(emb.cpu().numpy())\n            labels.extend(lbls.numpy())\n\n    return np.vstack(features), np.array(labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.726677Z","iopub.status.idle":"2026-01-15T17:42:28.727001Z","shell.execute_reply.started":"2026-01-15T17:42:28.726838Z","shell.execute_reply":"2026-01-15T17:42:28.726854Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Extract Training Embeddings (DATABASE)","metadata":{}},{"cell_type":"code","source":"train_features, train_labels = extract_features(feature_model, train_loader)\nprint(train_features.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.729193Z","iopub.status.idle":"2026-01-15T17:42:28.729583Z","shell.execute_reply.started":"2026-01-15T17:42:28.729382Z","shell.execute_reply":"2026-01-15T17:42:28.729401Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Similarity Computation (RECOMMENDATION ENGINE)","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.730523Z","iopub.status.idle":"2026-01-15T17:42:28.730900Z","shell.execute_reply.started":"2026-01-15T17:42:28.730730Z","shell.execute_reply":"2026-01-15T17:42:28.730748Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Recommend Function","metadata":{}},{"cell_type":"code","source":"def recommend(query_feature, feature_db, top_k=5):\n    sims = cosine_similarity(query_feature.reshape(1, -1), feature_db)\n    top_indices = sims[0].argsort()[-top_k-1:][::-1]\n    return top_indices\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.732295Z","iopub.status.idle":"2026-01-15T17:42:28.732754Z","shell.execute_reply.started":"2026-01-15T17:42:28.732493Z","shell.execute_reply":"2026-01-15T17:42:28.732532Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test Recommendation (VISUAL OUTPUT)\n### 10.1 Pick Query Image","metadata":{}},{"cell_type":"code","source":"query_img, _ = test_ds[10]\nquery_tensor = query_img.unsqueeze(0).to(device)\n\nwith torch.no_grad():\n    query_feature = feature_model(query_tensor).cpu().numpy()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.735173Z","iopub.status.idle":"2026-01-15T17:42:28.735526Z","shell.execute_reply.started":"2026-01-15T17:42:28.735359Z","shell.execute_reply":"2026-01-15T17:42:28.735376Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Get Similar Images ","metadata":{}},{"cell_type":"code","source":"indices = recommend(query_feature, train_features, top_k=5)\nindices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.737233Z","iopub.status.idle":"2026-01-15T17:42:28.737659Z","shell.execute_reply.started":"2026-01-15T17:42:28.737438Z","shell.execute_reply":"2026-01-15T17:42:28.737458Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Display Results","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12,4))\n\n# Query\nplt.subplot(1,6,1)\nplt.imshow(query_img.permute(1,2,0))\nplt.title(\"Query\")\nplt.axis('off')\n\n# Recommendations\nfor i, idx in enumerate(indices[:5]):\n    img_path = os.path.join(image_dir, train_df.iloc[idx]['image'])\n    img = Image.open(img_path)\n\n    plt.subplot(1,6,i+2)\n    plt.imshow(img)\n    plt.title(f\"Rec {i+1}\")\n    plt.axis('off')\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.739450Z","iopub.status.idle":"2026-01-15T17:42:28.740044Z","shell.execute_reply.started":"2026-01-15T17:42:28.739760Z","shell.execute_reply":"2026-01-15T17:42:28.739788Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# UI HTML CSS JS","metadata":{}},{"cell_type":"code","source":"from IPython.core.display import display, HTML\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.741457Z","iopub.status.idle":"2026-01-15T17:42:28.741893Z","shell.execute_reply.started":"2026-01-15T17:42:28.741709Z","shell.execute_reply":"2026-01-15T17:42:28.741728Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div class=\"container\">\n    <h1>Fashion Product Recommendation</h1>\n\n    <div class=\"upload-section\">\n        <input type=\"file\" id=\"queryImage\" accept=\"image/*\" />\n        <button onclick=\"showRecommendations()\">Recommend</button>\n    </div>\n\n    <div class=\"query-display\">\n        <h3>Query Image</h3>\n        <img id=\"queryImg\" src=\"\" alt=\"Query\" />\n    </div>\n\n    <div class=\"recommendations\">\n        <h3>Recommended Products</h3>\n        <div id=\"recGrid\" class=\"grid\"></div>\n    </div>\n</div>\n","metadata":{}},{"cell_type":"code","source":"from IPython.core.display import display, HTML\n\nhtml_code = \"\"\"\n<div class=\"container\">\n    <h1>Fashion Product Recommendation</h1>\n\n    <div class=\"upload-section\">\n        <input type=\"file\" id=\"queryImage\" accept=\"image/*\" />\n        <button id=\"recBtn\">Recommend</button>\n    </div>\n\n    <div class=\"query-display\">\n        <h3>Query Image</h3>\n        <img id=\"queryImg\" src=\"\" alt=\"Query\" />\n    </div>\n\n    <div class=\"recommendations\">\n        <h3>Recommended Products</h3>\n        <div id=\"recGrid\" class=\"grid\"></div>\n    </div>\n</div>\n\n<style>\nbody {\n    font-family: 'Arial', sans-serif;\n    background-color: #f5f5f5;\n    color: #333;\n}\n\n.container {\n    width: 90%;\n    margin: auto;\n    text-align: center;\n}\n\n.upload-section {\n    margin: 20px 0;\n}\n\n.query-display img {\n    width: 200px;\n    height: 200px;\n    object-fit: cover;\n    border-radius: 10px;\n    border: 2px solid #ddd;\n}\n\n.grid {\n    display: flex;\n    justify-content: center;\n    gap: 15px;\n    flex-wrap: wrap;\n    margin-top: 10px;\n}\n\n.grid img {\n    width: 150px;\n    height: 150px;\n    object-fit: cover;\n    border-radius: 8px;\n    box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n    transition: transform 0.3s, box-shadow 0.3s;\n}\n\n.grid img:hover {\n    transform: scale(1.05);\n    box-shadow: 0 8px 12px rgba(0,0,0,0.2);\n}\n</style>\n\n<script>\ndocument.getElementById('recBtn').onclick = function() {\n    const fileInput = document.getElementById('queryImage');\n    const queryImg = document.getElementById('queryImg');\n    const recGrid = document.getElementById('recGrid');\n\n    if (fileInput.files.length === 0) {\n        alert(\"Upload an image!\");\n        return;\n    }\n\n    // Show query image\n    queryImg.src = URL.createObjectURL(fileInput.files[0]);\n\n    // Clear previous recommendations\n    recGrid.innerHTML = \"\";\n\n    // Demo: show 5 images from dataset (update with real paths)\n    for (let i = 1; i <= 5; i++) {\n        const img = document.createElement('img');\n        img.src = \"/kaggle/input/fashion-product-images-dataset/fashion-dataset/images/\" + (39400 + i) + \".jpg\"; // Replace with real recommended IDs\n        recGrid.appendChild(img);\n    }\n};\n</script>\n\"\"\"\n\ndisplay(HTML(html_code))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:28.743415Z","iopub.status.idle":"2026-01-15T17:42:28.743797Z","shell.execute_reply.started":"2026-01-15T17:42:28.743620Z","shell.execute_reply":"2026-01-15T17:42:28.743639Z"}},"outputs":[],"execution_count":null}]}